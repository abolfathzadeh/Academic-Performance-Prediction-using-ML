# Academic-Performance-Prediction-using-ML
Academic Performance Prediction using Machine learning algorithms.

## describtion
The objective of the study is to use a method to predict student performance during the semesters and to compare accuracy perceptron for a dataset of student performance. In this regard, Machine Learning techniques were applied to the student performance dataset provided by the Kaggle.com website. Multilayer Perceptron, Random Forest, SVM, Na√Øve Bayes, Decision tree and K-NN algorithms were used to predict the Grade result of students as a factor of performance. The Student Performance dataset is used to forecast how well students will perform in their tests. As a result, with 94.9% accuracy, the results were predicted.


##dataset 
The dataset used is Students Performance which was collected from the Kaggle.com website, about gender, past education, jobs, facilities, health situations and exam preparations of students. Jupyter notebook was the platform used to implement the algorithms.
All process of cleaning the data was done on the data before applying the models and converting the mass data to clear one


 
## depicting the destiny of the Grade. 
![image](https://user-images.githubusercontent.com/9671082/229552600-6631d281-732d-44b7-a047-4d128722a376.png)


## Pseudocode of algorithm:
1.	Begin the Program
2.	Import the relevant libraries.
3.	read the CSV-formatted dataset file.
4.	Cleaning the data.
5.	Encoding the categorical columns
6.	Feature selection by SelectKBest
7.	loading the data to train and test set.
8.	Normalizing(Scaling) the Numerical Column values
9.	Use different Models.
10.	Evaluate Models
11.	Tune Models to find the best parameters.
12.	Print the accuracy scores to find the best model.
13.	Finish the programme.

## 4	RESULTS
Table 1 shows descriptive statistics of accuracy for all Algorithms. Table 1 also shows which algorithms give the low Mean Absolute Error, Mean Square Error and Round Mean Square Error. As it is depicted the Accuracy Score of the Random Forest with 94.9% was the best. In addition, the mean absolute error followed the same pattern with 0.05 by Random Forest as the lowest and 0.29 as the highest one by K-NN. 

![image](https://user-images.githubusercontent.com/9671082/229552979-a5b0d667-5d3c-45b8-8817-b7d95b55cb9d.png)

